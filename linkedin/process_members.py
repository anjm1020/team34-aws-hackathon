import json
import os
import sys
import subprocess
from pathlib import Path

sys.path.append('linkedin-scraper-mcp')
from postgres_config import PostgreSQLClient

def is_valid_linkedin_url(url):
    """ìœ íš¨í•œ LinkedIn URLì¸ì§€ í™•ì¸"""
    if not url or url.strip() == "":
        return False
    return "linkedin.com/in/" in url and "feed" not in url

def scrape_linkedin(url):
    """LinkedIn í”„ë¡œí•„ ìŠ¤í¬ë˜í•‘"""
    try:
        # URL ì •ê·œí™”
        normalized_url = normalize_url(url)
        result = subprocess.run(
            ["python", "main.py", normalized_url],
            capture_output=True,
            text=True,
            timeout=120,
            cwd="/Users/wa/golbob/aws/linkedin/linkedin-scraper-mcp"
        )
        
        if result.returncode == 0:
            print(f"âœ… LinkedIn ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {url}")
            return True
        else:
            print(f"âŒ LinkedIn ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {url}")
            return False
    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        return False

def find_linkedin_data(url, scraped_dir):
    """ìŠ¤í¬ë˜í•‘ëœ LinkedIn ë°ì´í„° ì°¾ê¸°"""
    for file_path in Path(scraped_dir).glob("*.json"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if 'url' in data and normalize_url(data['url']) == normalize_url(url):
                    return data
        except:
            continue
    return None

def normalize_url(url):
    """URL ì •ê·œí™”"""
    if not url:
        return ""
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    return url.rstrip('/')

def create_member_json(member_data, user_id, linkedin_data=None):
    """ë©¤ë²„ JSON ìƒì„±"""
    # LinkedIn ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ JSON í¬ë§· ìƒì„±
    if linkedin_data is None:
        linkedin_data = {
            "url": "",
            "name": "",
            "headline": "",
            "location": "",
            "about": "",
            "experiences": [],
            "education": [],
            "skills": [],
            "websites": [],
            "email": None
        }
    
    return {
        "user_id": user_id,
        "original_slack_id": member_data.get("user_id", ""),
        "survey_data": {
            "linkedin_url": member_data.get("linkedin_url", ""),
            "job_field": member_data.get("job_field", ""),
            "interests": member_data.get("interests", ""),
            "hobbies": member_data.get("hobbies", ""),
            "timestamp": member_data.get("timestamp", "")
        },
        "linkedin_data": linkedin_data
    }

def upload_to_db(json_file_path):
    """DBì— ì—…ë¡œë“œ"""
    with open(json_file_path, 'r', encoding='utf-8') as f:
        member_data = json.load(f)
    
    db_client = PostgreSQLClient()
    if not db_client.connect():
        return False
    
    try:
        result = db_client.insert_profile_with_user_id(member_data)
        return result is not None
    except Exception as e:
        print(f"âŒ DB ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    finally:
        db_client.close()

def process_members(survey_json_path):
    """ì „ì²´ ë©¤ë²„ ì²˜ë¦¬ í”Œë¡œìš°"""
    
    # 1. ì„¤ë¬¸ ë°ì´í„° ë¡œë“œ
    with open(survey_json_path, 'r', encoding='utf-8') as f:
        survey_data = json.load(f)
    
    # í´ë” ìƒì„±
    Path("result").mkdir(exist_ok=True)
    scraped_dir = "linkedin-scraper-mcp/scraped_data"
    
    print(f"ğŸš€ ì´ {len(survey_data)}ëª…ì˜ ë©¤ë²„ ì²˜ë¦¬ ì‹œì‘")
    
    for i, member in enumerate(survey_data, 1):
        user_id = i
        linkedin_url = member.get("linkedin_url", "")
        
        print(f"\nğŸ‘¤ ë©¤ë²„ {user_id} ì²˜ë¦¬ ì¤‘...")
        
        linkedin_data = None
        
        # 2. LinkedIn URL í™•ì¸ ë° ì²˜ë¦¬
        if is_valid_linkedin_url(linkedin_url):
            print(f"ğŸ”— LinkedIn URL ë°œê²¬: {linkedin_url}")
            
            # ê¸°ì¡´ ìŠ¤í¬ë˜í•‘ ë°ì´í„° í™•ì¸
            linkedin_data = find_linkedin_data(linkedin_url, scraped_dir)
            
            if not linkedin_data:
                print("ğŸ“¥ ìƒˆë¡œìš´ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
                if scrape_linkedin(linkedin_url):
                    linkedin_data = find_linkedin_data(linkedin_url, scraped_dir)
        else:
            print("ğŸ“ LinkedIn URL ì—†ìŒ - ì„¤ë¬¸ ë°ì´í„°ë§Œ ì €ì¥")
        
        # 3. ë©¤ë²„ JSON ìƒì„±
        member_json = create_member_json(member, user_id, linkedin_data)
        
        # 4. result í´ë”ì— ì €ì¥
        result_file = f"result/{user_id}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(member_json, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {result_file}")
        
        # 5. DB ì—…ë¡œë“œ (í˜„ì¬ ë¹„í™œì„±í™”)
        # if upload_to_db(result_file):
        #     print(f"âœ… DB ì—…ë¡œë“œ ì„±ê³µ: User {user_id}")
        # else:
        #     print(f"âŒ DB ì—…ë¡œë“œ ì‹¤íŒ¨: User {user_id}")
        print(f"ğŸ“ JSON ìƒì„± ì™„ë£Œ: User {user_id}")
    
    print(f"\nğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ! {len(survey_data)}ëª…")

if __name__ == "__main__":
    survey_file = "converted_json/networking.json"
    process_members(survey_file)